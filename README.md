# LLMs – the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on Indian Court Cases
This repository contains the relevant data and codes for the paper 'LLMs – the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on Indian Court Cases' accepted to the Findings of the [EMNLP 2023](https://2023.emnlp.org) conference.

Here's a directory tree.

```
LLM_Legal_Prompt_Generation
├── Judgment Prediction
│   ├── LLM
│   │   ├── Codes
│   │   │   ├── jp.py
│   │   │   ├── jpe.py
│   │   ├── Datasets
│   │   │   ├── JP.csv
│   │   │   ├── JPE.csv
│   │   │   ├── JPE_with_pet_res.csv
│   │   │   ├── JP_with_pet_res.csv
│   │   ├── readme.md
│   ├── Transformer based Models
│   │   ├── Codes
│   │   │   ├── Evalution on ILDC expert dataset.ipynb
│   │   │   ├── Legal_judgment_training_with_transformers.py
│   │   ├── Datasets
│   │   │   ├── readme.md
│   ├── surname_wordlist
│   │   ├── hindu_surname_file.txt
│   │   ├── muslim_surname_file.txt
├── Statute Prediction
│   ├── Baseline Models
│   │   ├── data_generator.py
│   │   ├── evaluate.py
│   │   ├── metrics.py
│   │   ├── train.py
│   │   ├── utils.py
│   │   ├── Model
│   │   │   ├── Multi-label Classification
│   │   │   │   ├── net.py
│   │   │   ├── Binary Classification
│   │   │   │   ├── net.py
│   │   ├── Experiments
│   │   │   ├── params
│   │   │   │   ├── params_inlegalbert.json
│   │   │   │   ├── params_legalbert.json
│   │   │   │   ├── params_xlnet.json
│   ├── LLM
│   │   ├── Codes
│   │   │   ├── ALL TASK CODE.ipynb
│   │   │   ├── ALL TASK CODE.py
│   │   ├── Datasets
│   │   │   ├── 13_Cases_Gender and Bias Prediction_with explanations.csv
│   │   │   ├── 245cases.csv
│   │   │   ├── Gender and Religion Bias cases.csv
│   │   │   ├── query.csv
│   │   │   ├── statute_pred_100_cases_without_exp-gender_religion_bias.csv
│   │   │   ├── statute_pred_100_cases_without_exp.csv
│   │   │   ├── statute_pred_45_cases_with_exp.csv
│   │   │   ├── statute_pred_45_cases_without_exp.csv
│   │   ├── readme.md
├── README.md
```
## Abstract
The **Large Language Models (LLMs)** have impacted many real-life tasks. To examine the efficacy of LLMs in a high-stake domain like law,
we have applied state-of-the-art LLMs for two
popular tasks: **Statute Prediction** and **Judgment
Prediction**, on Indian Supreme Court cases. We
see that while LLMs exhibit excellent predictive performance in Statute Prediction, their
performance dips in Judgment Prediction when
compared with many standard models. The
explanations generated by LLMs (along with
prediction) are of moderate to decent quality.
We also see evidence of gender and religious
bias in the LLM-predicted results. In addition,
we present a note from a senior legal expert
on the ethical concerns of deploying LLMs in
these critical legal tasks.
## Authors
Shaurya Vats, Atharva Zope, Somsubhra De, Anurag Sharma, Upal Bhattacharya, Shubham Kumar Nigam, Shouvik Kumar Guha, Koustav Rudra, Kripabandhu Ghosh
## Citation
```
@inproceedings{vats-etal-2023-llms,
    title = "{LLM}s {--} the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on {I}ndian Court Cases",
    author = "Vats, Shaurya  and
      Zope, Atharva  and
      De, Somsubhra  and
      Sharma, Anurag  and
      Bhattacharya, Upal  and
      Nigam, Shubham  and
      Guha, Shouvik  and
      Rudra, Koustav  and
      Ghosh, Kripabandhu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.831",
    pages = "12451--12474",
    abstract = "The Large Language Models (LLMs) have impacted many real-life tasks. To examine the efficacy of LLMs in a high-stake domain like law, we have applied state-of-the-art LLMs for two popular tasks: Statute Prediction and Judgment Prediction, on Indian Supreme Court cases. We see that while LLMs exhibit excellent predictive performance in Statute Prediction, their performance dips in Judgment Prediction when compared with many standard models. The explanations generated by LLMs (along with prediction) are of moderate to decent quality. We also see evidence of gender and religious bias in the LLM-predicted results. In addition, we present a note from a senior legal expert on the ethical concerns of deploying LLMs in these critical legal tasks.",
}
```
## Contact Us
Feel free to write your queries or questions to `kripaghosh[at]iiserkol[dot]ac[dot]in`.
